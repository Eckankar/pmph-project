\documentclass[a4paper]{article}
\usepackage{a4wide}

\usepackage[danish,english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[protrusion=true,expansion=true,final]{microtype}
\usepackage[T1]{fontenc}
\usepackage[sc]{mathpazo}

\usepackage{hyperref}
\usepackage{booktabs}

\usepackage{todonotes}

\usepackage{subfig}
\newcommand{\subfigureautorefname}{\figureautorefname}
\newcommand{\subtableautorefname}{\tableautorefname}
\usepackage[margin=10pt,font=footnotesize,labelfont=bf]{caption}

\usepackage{textcomp}
\usepackage{listingsutf8}
\lstset{
    keywordstyle=\bfseries\ttfamily\color[rgb]{0,0,1},
    identifierstyle=\ttfamily,
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\ttfamily\color[rgb]{0.627,0.126,0.941},
    showstringspaces=false,
    basicstyle=\footnotesize\ttfamily,
    numbersep=10pt,
    tabsize=2,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    breakatwhitespace=false,
    aboveskip={1.5\baselineskip},
    columns=fixed,
    extendedchars=true,
    frame=single,
    captionpos=b,
    language=C++
}

\title{Programming Massively Parallel Hardware \\ Group Project}
\author{Sebastian Paaske TÃ¸rholm \and Daniel Egeberg}

\begin{document}
\maketitle

\section{Loop accesses}
\label{sec:loop}

\subsection{\texttt{run\_OrigCPU}}

Contains a single loop that can be parallelized by privatizing \texttt{strike} and \texttt{globs}.

\subsection{\texttt{value}}

The loop in this function is not parallelizable. It loops over time steps, and
calls \texttt{rollback}, which uses results from previous time steps. This is
sequential in nature.

\subsection{\texttt{updateParams}}

Already parallelizable as each iteration step is independent.

\subsection{\texttt{rollback}}

Contains four outer loops:
\begin{enumerate}
    \item Already parallelizable as it writes to \texttt{u[j][i]} without reading from \texttt{u}.
    \item Already parallelizable as it writes to \texttt{v[i][j]} and \texttt{u[j][i]} without reading from \texttt{v} or \texttt{u} for different indices.
    \item The inner loop is already parallelizable as it writes to separate indices of \texttt{a}, \texttt{b} and \texttt{c}. The outer loop cannot be parallelized as it calls \texttt{tridag}, which rewrites \texttt{u} based on previous iterations.
    \item The inner loop is already parallelizable as it writes to separate indices of \texttt{a}, \texttt{b}, \texttt{c} and \texttt{y}. The outer loop can, unlike the previous loop, be parallelized by privatizing the aforementioned variables. This is possible because we write to \texttt{myResult} based on \texttt{y} (which is local).
\end{enumerate}

\subsection{\texttt{tridag}}

This can be parallelized by changing the algorithm to the one mentioned in the lectures.


\section{OpenMP implementation}

This is done by privatizing the loop in \texttt{run\_OrigCPU} as described in
\autoref{sec:loop}. Doing this we can just add \autoref{lst:ompPragma} to the
outer loop as suggested.\todo{Argue why this is valid.}

\begin{lstlisting}[caption={OpenMP parallelization pragma.},label={lst:ompPragma}]
#pragma omp parallel for default(shared) schedule(static) if(outer>4)
\end{lstlisting}


\section{CUDA implementation}

\subsection{Moving out the time loop}

As we observed in \autoref{sec:loop}, the time loop is sequential, so we want
this as our outermost loop. We start by inlining the \texttt{value} function in
the run function. This enables us to perform this rewrite. From this, we can
observe that \texttt{updateParams} and \texttt{rollback} are the only
operations that depend on the time step. Thus, we can factor out all other
operations in a parallelizable loop by maintaining a separate \texttt{globs}
struct per ``\texttt{outer}''.

\subsection{Converting from STL vectors}

The original implementation uses the \texttt{PrivGlobs} struct, which is
implemented using vectors from C++ STL\@. We cannot use things from STL in a
CUDA kernel, and in particular, we cannot use the vectors as they use dynamic
resizing, while dynamic memory allocation is not possible in a CUDA kernel.
This requires us to use arrays directly. We replace the \texttt{PrivGlobs}
array with the individual fields extended with the \texttt{outer} dimension.
This means that \autoref{lst:vector1} can be rewritten to
\autoref{lst:vector2}.

\begin{lstlisting}[caption={An array of \texttt{PrivGlobs}.},label={lst:vector1}]
PrivGlobs globs[outer];
\end{lstlisting}
\begin{lstlisting}[caption={Expanding the struct.},label={lst:vector2}]
REAL myX[outer][numX];
REAL myY[outer][numY];
// ...
\end{lstlisting}

\subsection{Eliminate duplicate computations}
\label{sec:eliminate_dup_comp}

Many variables are only written to once after the initial write (see
\autoref{tbl:rw}), and are not a function of \texttt{outer}'s iteration
variable. This allows to move these computations outside the outer loop. An
example of such a transformation is \autoref{lst:duplicate1} to
\autoref{lst:duplicate2}. This kind of transformation is valid for
\texttt{myX}, \texttt{myY}, \texttt{myTimeline}, \texttt{myDxx} and
\texttt{myDyy}. This saves computation time and memory.

\begin{table}
    \centering
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Function}             & \texttt{myX} & \texttt{myY} & \texttt{myTimeline} & \texttt{myResult} & \texttt{myVarX} & \texttt{myVarY} & \texttt{myDxx} & \texttt{myDyy} \\
        \midrule
        \texttt{initGrid}             & W            & W            & W                   &                   &                 &                 &                &                \\
        \texttt{initOperator}, x      & R            &              &                     &                   &                 &                 & W              &                \\
        \texttt{initOperator}, y      &              & R            &                     &                   &                 &                 &                & W              \\
        \texttt{setPayoff}            & R            &              &                     & W                 &                 &                 &                &                \\
        \texttt{updateParams}         & R            & R            & R                   &                   & W               & W               &                &                \\
        \texttt{rollback}, explicit x &              &              &                     & R                 & R               &                 & R              &                \\
        \texttt{rollback}, explicit y &              &              &                     & R                 &                 & R               &                & R              \\
        \texttt{rollback}, implicit x &              &              &                     &                   & R               &                 & R              &                \\
        \texttt{rollback}, implicit y &              &              &                     & W                 &                 & R               &                & R              \\
        \bottomrule
    \end{tabular}
    \caption{Reads and writes in global variables.}
    \label{tbl:rw}
\end{table}

\begin{lstlisting}[caption={Code doing duplicate computations.},label={lst:duplicate1}]
REAL myX[outer][numX];
REAL myY[outer][numY];
// ...
for (unsigned i = 0; i < outer; ++i) {
    initGrid(..., i, myX, myY, numX, numY, ...);
    // ...
}
\end{lstlisting}
\begin{lstlisting}[caption={Removal of duplicate computations.},label={lst:duplicate2}]
REAL myX[numX];
REAL myY[numY];
// ...
initGrid(..., myX, myY, numX, numY, ...);
// ...
\end{lstlisting}

\subsection{Converting to kernels}

Each line in \autoref{tbl:rw} gets converted to a separate CUDA kernel taking
the parameters where it has read/write operations. Here we need to take special
care of \texttt{rollback} as it has a number of local variables that are shared
across the different subparts of the function. These are \texttt{u} and
\texttt{v}, and their read/write operations are outlined in
\autoref{tbl:rw_rollback}. As we cannot have dynamically sized local variables,
we allocate these globally using \texttt{cudaMalloc}. The same happens for
\texttt{a}, \texttt{b}, \texttt{c} and \texttt{y}, which are needed inside the
four rollback kernels.

This logically represents splitting up the outer loop, i.e.\ doing the
transformation \autoref{lst:splitting1} to \autoref{lst:splitting2}. This
transformation is valid as there are no dependencies across \texttt{outer}
iterations and the results are saved per iteration, except where the results
are identical each iteration as shown in \autoref{sec:eliminate_dup_comp}.

\begin{lstlisting}[caption={Before splitting loops.},label={lst:splitting1}]
for (unsigned i = 0; i < outer; ++i) {
    updateparams(...);
    rollback_explicit_x(...);
    // ...
}
\end{lstlisting}
\begin{lstlisting}[caption={After splitting loops.},label={lst:splitting2}]
for (unsigned i = 0; i < outer; ++i) {
    updateparams(...);
}
for (unsigned i = 0; i < outer; ++i) {
    rollback_explicit_x(...);
}
for (unsigned i = 0; i < outer; ++i) {
    // ...
}
\end{lstlisting}

\begin{table}
    \centering
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Function}             & \texttt{u} & \texttt{v} & \texttt{myResult} & \texttt{myVarX} & \texttt{myVarY} & \texttt{myDxx} & \texttt{myDyy} \\
        \midrule
        \texttt{rollback}, explicit x & W          &            & R                 & R               &                 & R              &                \\
        \texttt{rollback}, explicit y & RW         & W          & R                 &                 & R               &                & R              \\
        \texttt{rollback}, implicit x & RW         &            &                   & R               &                 & R              &                \\
        \texttt{rollback}, implicit y & R          & R          & W                 &                 & R               &                & R              \\
        \bottomrule
    \end{tabular}
    \caption{Reads and writes for \texttt{rollback}.}
    \label{tbl:rw_rollback}
\end{table}

Creating the kernels is done by parallelizing loops as block/thread dimensions
in the kernel. An example is \texttt{rollback}'s ``explicit x'' loop in
\autoref{lst:kernel1} which becomes the CUDA kernel in \autoref{lst:kernel2}.

\begin{lstlisting}[caption={The original ``explicit x'' loop in \texttt{rollback}.},label={lst:kernel1}]
for (unsigned o = 0; o < outer; ++o) {
    // explicit x (rollback)
    for (i = 0; i < numX; i++) {
        for (j = 0; j < numY; j++) {
            // perform computation
        }
    }
}
\end{lstlisting}

\begin{lstlisting}[caption={Converting \texttt{rollback}'s ``explicit x'' loop to a CUDA kernel.},label={lst:kernel2}]
__global__
void rollback_explicit_x_kernel(...) {
    unsigned int tid_y = blockIdx.x*blockDim.x + threadIdx.x;
    unsigned int tid_x = blockIdx.y*blockDim.y + threadIdx.y;

    if (tid_y >= numY || tid_x >= numX)
        return;

    for (unsigned o = 0; o < outer; ++o) {
        // perform computation
    }
}
\end{lstlisting}

Seeing as we cannot pass real multi-dimensional arrays to CUDA kernels, we have
to create one-dimensional arrays and calculate the index manually based on
their coordinates dimension sizes. For this we have created the macros in
\autoref{lst:idx_macro}. This means that if \texttt{a} is a $3 \times 4 \times
5$-dimensional array, we would write \verb|a[IDX3(3,4,5, i,j,k)]| instead of
\verb|a[i][j][k]|.

\begin{lstlisting}[caption={Calculating array indices.},label={lst:idx_macro}]
#define IDX2(DIMX, DIMY, X, Y)           ((X)*(DIMY) + (Y))
#define IDX3(DIMX, DIMY, DIMZ, X, Y, Z)  ((X)*(DIMY)*(DIMZ) + (Y)*(DIMZ) + (Z))
\end{lstlisting}

\subsection{Coalescing memory accesses}

In order to coalesce memory accesses, we restructured a number of the arrays,
to match the loop nesting in our kernels, such as in \autoref{lst:coalesce1}
that becomes \autoref{lst:coalesce2}.

\begin{lstlisting}[caption={\texttt{rollback}'s ``explicit x'' kernel pre-coalescing.},label={lst:coalesce1}]
void rollback_explicit_x_kernel(...) {
    unsigned int tid_y = blockIdx.x*blockDim.x + threadIdx.x;
    unsigned int tid_x = blockIdx.y*blockDim.y + threadIdx.y;

    if (tid_y >= numY || tid_x >= numX)
        return;

    for (unsigned o = 0; o < outer; ++o) {
        // ...
        u[IDX3(outer,numY,numX, o,tid_y,tid_x)] = ...
        // ...
    }
\end{lstlisting}

\begin{lstlisting}[caption={\texttt{rollback}'s ``explicit x'' kernel post-coalescing.},label={lst:coalesce2}]
void rollback_explicit_x_kernel(...) {
    unsigned int tid_y = blockIdx.x*blockDim.x + threadIdx.x;
    unsigned int tid_x = blockIdx.y*blockDim.y + threadIdx.y;

    if (tid_y >= numY || tid_x >= numX)
        return;

    for (unsigned o = 0; o < outer; ++o) {
        // ...
        u[IDX3(outer,numX,numY, o,tid_x,tid_y)] = ...
        // ...
    }
\end{lstlisting}

In order to do this restructuring, we can do a transpose of the two innermost
dimensions. With most of the arrays, we only need them in their transposed
form in order to get good coalesced access, and therefore don't need to
perform any actual transpose operation.

The transpose function is based on the tiled two-dimensional transpose,
performing a segmented transpose, treating each row as a segment.

Furthermore, we split the implicit kernels into two parts, the first
initializing the input vectors to tridag, and the other performing the
actual tridag call. The first kernel is then parallelizable in the x and y
dimensions, allowing us to do nicely coalesced writes. The second kernel is
only parallelizable on one of x and y.

Coalescing memory in all the kernels, without optimizing tridag brought the
running time on the large dataset down from about 25 seconds, to about 16
seconds.

\subsection{Optimizing tridag}

In order to coalesce memory accesses in tridag, we needed to change it to work
on columns instead of rows. We enable doing this by resizing each input vector
to have size \texttt{numZ = max(numX, numY)}, allowing us to access the element
\texttt{i} at index \texttt{i * numZ}. This is done by transforming
\autoref{lst:tridag1} to \autoref{lst:tridag2}.

\begin{lstlisting}[caption={The call to \texttt{tridag} pre-coalescing},label={lst:tridag1}]
__device__
void tridag(...) {
    // ...
    uu[i] = b[i] - beta*c[i-1];
    // ...
}

__global__
void rollback_implicit_x_part2_kernel(...) {
    // ...
    REAL *myA  =  &a[IDX3(outer,numZ,numZ, tid_outer,tid_y,0)];
    REAL *myB  =  &b[IDX3(outer,numZ,numZ, tid_outer,tid_y,0)];
    // ...

    tridag(myA,myB,myC,myU,numX,myU,myYY);
}
\end{lstlisting}

\begin{lstlisting}[caption={The call to \texttt{tridag} post-coalescing},label={lst:tridag2}]
__device__
void tridag(..., int stride) {
    // ...
    uu[i*stride] = b[i*stride] - beta*c[(i-1)*stride];
    // ...
}

__global__
void rollback_implicit_x_part2_kernel(...) {
    // ...
    REAL *myA  =  &a[IDX3(outer,numZ,numZ, tid_outer,0,tid_y)];
    REAL *myB  =  &b[IDX3(outer,numZ,numZ, tid_outer,0,tid_y)];
    // ...

    tridag(myA,myB,myC,myU,numX,myU,myYY,numZ);
}
\end{lstlisting}

Performing this coalescing had a large impact on the execution time, lowering
it from about 16 seconds to around 9 seconds on the large dataset.

While we do believe this transformation to be valid, sadly we appear to have
introduced a bug in performing it, making the precision degenerate rapidly
on the large dataset, causing it to not validate, even with our increased
epsilon. We do believe that the execution time matches what we could expect
with the bug fixed, however.

\subsection{Result validation}

Prior to tridag coalescing, we have been unable to get the CUDA version to
validate at the supplied $\varepsilon$\footnote{Except on the small dataset.}. It
does, however, validate at $10 \cdot \varepsilon$. Due to the result being that
close to validating, we believe our fundamental methods to be sound. We have
sadly been unable to locate the bug(s) causing the imprecision in our result,
in spite on numerous hours of debugging.

In performing coalescing on tridag, we seem to have introduced further
imprecision. Inspection has shown \texttt{u} to be precise to only around
$10^{-7}$ on the first iteration, causing an imprecision we believe to
escalate as the time step increases.

\subsection{Performance}

% small:
% orig: 2018757, 2457320, 2120604 (2198894, 229524.1)
% omp: 187334, 189035, 191612 (189327, 2153.896)
% cuda (no tridag): 1774632, 1566203, 1572650 (1637828, 118519.3)
% cuda (with tridag): 1335515, 1327069, 1315693 (1326092, 9947.026)

% medium:
% orig: 4704631, 4665264, 4448265 (4606053, 138059.1)
% omp: 247887, 232813, 232282 (237660.7, 8860.243)
% cuda (no tridag): 1791254, 1781069, 1777709 (1783344, 7053.26)
% cuda (with tridag): 1407254, 1393493, 1409386 (1403378, 8626.49)

% large:
% orig: 204725583, 190447498, 190311103 (195161395, 8283111)
% omp: 9056566, 9115644, 9157951 (9110054, 50923.16)
% cuda (no tridag): 16576421, 16588736, 16516714 (16560624, 38522.19)
% cuda (with tridag): 8025731, 8031441, 8029736 (8028969, 2931.187)

We observe the timings in tables~\ref{tbl:time_small},~\ref{tbl:time_medium}
and~\ref{tbl:time_large}. We measured the time based on three runs, and the
reported times are the mean runtimes with standard deviations. Unfortunately,
our CUDA version is only faster than the OpenMP version on the large dataset,
although it's a marginal speedup.

\begin{table}
    \centering
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Implementation} & \textbf{Time (s)} & \textbf{Step speedup} & \textbf{Cumulative speedup} \\
        \midrule
        Original         & $2.20\pm0.23$ &       &       \\
        OpenMP           & $0.19\pm0.00$ & 11.6x & 11.6x \\
        CUDA (wo/tridag) & $1.64\pm0.12$ &  0.1x &  1.3x \\
        CUDA (w/tridag)  & $1.33\pm0.01$ &  1.2x &  1.7x \\
        \bottomrule
    \end{tabular}
    \caption{Performance on the small dataset.}
    \label{tbl:time_small}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Implementation} & \textbf{Time (s)} & \textbf{Step speedup} & \textbf{Cumulative speedup} \\
        \midrule
        Original         & $4.61\pm0.14$ &       &       \\
        OpenMP           & $0.24\pm0.01$ & 19.2x & 19.2x \\
        CUDA (wo/tridag) & $1.78\pm0.01$ &  0.1x &  2.6x \\
        CUDA (w/tridag)  & $1.40\pm0.01$ &  1.3x &  3.3x \\
        \bottomrule
    \end{tabular}
    \caption{Performance on the medium dataset.}
    \label{tbl:time_medium}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Implementation} & \textbf{Time (s)} & \textbf{Step speedup} & \textbf{Cumulative speedup} \\
        \midrule
        Original         & $195.16\pm8.28$ &       &       \\
        OpenMP           &   $9.11\pm0.05$ & 21.4x & 21.4x \\
        CUDA (wo/tridag) &  $16.56\pm0.04$ &  0.6x & 11.8x \\
        CUDA (w/tridag)  &   $8.03\pm0.00$ &  2.1x & 24.3x \\
        \bottomrule
    \end{tabular}
    \caption{Performance on the large dataset.}
    \label{tbl:time_large}
\end{table}


\subsection{Possible improvements}

This section will outline possible improvements that could be made to our
program, that we haven't gotten done due to time constaints.

Currently our implementation only performs parallellization on two dimensions
of the dataset. For small datasets, this may not provide enough parallellism
to fully utilize the hardware available. To improve on this situation, we
could write a simple heuristic to detect if there isn't sufficient parallism
in the given dimensions, and if not, parallelize on all 3 parallelizable
dimensions. It should be possible with relatively little effort, to make
versions of our existing kernels that do their work in three dimensions
instead of two.

The tridag implementation we use is simply a coalesced version of the original
one. Writing a tridag using scans could most likely help improve performance
further in the smaller cases, and possibly also in the larger case.

\end{document}
