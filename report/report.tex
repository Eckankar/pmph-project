\documentclass[a4paper]{article}
\usepackage{a4wide}

\usepackage[danish,english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[protrusion=true,expansion=true,final]{microtype}
\usepackage[T1]{fontenc}
\usepackage[sc]{mathpazo}

\usepackage{hyperref}
\usepackage{booktabs}

\usepackage{todonotes}

\usepackage{subfig}
\newcommand{\subfigureautorefname}{\figureautorefname}
\newcommand{\subtableautorefname}{\tableautorefname}
\usepackage[margin=10pt,font=footnotesize,labelfont=bf]{caption}

\usepackage{textcomp}
\usepackage{listingsutf8}
\lstset{
    keywordstyle=\bfseries\ttfamily\color[rgb]{0,0,1},
    identifierstyle=\ttfamily,
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\ttfamily\color[rgb]{0.627,0.126,0.941},
    showstringspaces=false,
    basicstyle=\footnotesize\ttfamily,
    numbersep=10pt,
    tabsize=2,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    breakatwhitespace=false,
    aboveskip={1.5\baselineskip},
    columns=fixed,
    extendedchars=true,
    frame=single,
    captionpos=b,
    language=C++
}

\title{Programming Massively Parallel Hardware \\ Group Project}
\author{Sebastian Paaske TÃ¸rholm \and Daniel Egeberg}

\begin{document}
\maketitle

\section{Loop accesses}
\label{sec:loop}

\subsection{\texttt{run\_OrigCPU}}

Contains a single loop that can be parallelized by privatizing \texttt{strike} and \texttt{globs}.

\subsection{\texttt{value}}

The loop in this function is not parallelizable. It loops over time steps, and
calls \texttt{rollback}, which uses results from previous time steps. This is
sequential in nature.

\subsection{\texttt{updateParams}}

Already parallelizable as each iteration step is independent.

\subsection{\texttt{rollback}}

Contains four outer loops:
\begin{enumerate}
    \item Already parallelizable as it writes to \texttt{u[j][i]} without reading from \texttt{u}.
    \item Already parallelizable as it writes to \texttt{v[i][j]} and \texttt{u[j][i]} without reading from \texttt{v} or \texttt{u} for different indices.
    \item The inner loop is already parallelizable as it writes to separate indices of \texttt{a}, \texttt{b} and \texttt{c}. The outer loop cannot be parallelized as it calls \texttt{tridag}, which rewrites \texttt{u} based on previous iterations.
    \item The inner loop is already parallelizable as it writes to separate indices of \texttt{a}, \texttt{b}, \texttt{c} and \texttt{y}. The outer loop can, unlike the previous loop, be parallelized by privatizing the aforementioned variables. This is possible because we write to \texttt{myResult} based on \texttt{y} (which is local).
\end{enumerate}

\subsection{\texttt{tridag}}

This can be parallelized by changing the algorithm to the one mentioned in the lectures.


\section{OpenMP implementation}

This is done by privatizing the loop in \texttt{run\_OrigCPU} as described in
\autoref{sec:loop}. Doing this we can just add \autoref{lst:ompPragma} to the
outer loop as suggested.\todo{Argue why this is valid.}

\begin{lstlisting}[caption={bla bla},label={lst:ompPragma}]
#pragma omp parallel for default(shared) schedule(static) if(outer>4)
\end{lstlisting}


\section{CUDA implementation}

\subsection{Moving out the time loop}

As we observed in \autoref{sec:loop}, the time loop is sequential, so we want
this as our outermost loop. We start by inlining the \texttt{value} function in
the run function. This enables us to perform this rewrite. From this, we can
observe that \texttt{updateParams} and \texttt{rollback} are the only
operations that depend on the time step. Thus, we can factor out all other
operations in a parallelizable loop by maintaining a separate \texttt{globs}
struct per ``\texttt{outer}''.

\subsection{Converting from STL vectors}

The original implementation uses the \texttt{PrivGlobs} struct, which is
implemented using vectors from C++ STL\@. We cannot use things from STL in a
CUDA kernel, and in particular, we cannot use the vectors as they use dynamic
resizing, while dynamic memory allocation is not possible in a CUDA kernel.
This requires us to use arrays directly. We replace the \texttt{PrivGlobs}
array with the individual fields extended with the \texttt{outer} dimension.
This means that \autoref{lst:vector1} can be rewritten to
\autoref{lst:vector2}.

\begin{lstlisting}[caption={An array of \texttt{PrivGlobs}.},label={lst:vector1}]
PrivGlobs globs[outer];
\end{lstlisting}
\begin{lstlisting}[caption={Expanding the struct.},label={lst:vector2}]
REAL myX[outer][numX];
REAL myY[outer][numY];
// ...
\end{lstlisting}

\subsection{Eliminate duplicate computations}
\label{sec:eliminate_dup_comp}

Many variables are only written to once after the initial write (see
\autoref{tbl:rw}), and are not a function of \texttt{outer}'s iteration
variable. This allows to move these computations outside the outer loop. An
example of such a transformation is \autoref{lst:duplicate1} to
\autoref{lst:duplicate2}. This kind of transformation is valid for
\texttt{myX}, \texttt{myY}, \texttt{myTimeline}, \texttt{myDxx} and
\texttt{myDyy}. This saves computation time and memory.

\begin{table}
    \centering
    \begin{tabular}{lcccccccc}
        \toprule
        \textbf{Function}             & \texttt{myX} & \texttt{myY} & \texttt{myTimeline} & \texttt{myResult} & \texttt{myVarX} & \texttt{myVarY} & \texttt{myDxx} & \texttt{myDyy} \\
        \midrule
        \texttt{initGrid}             & W            & W            & W                   &                   &                 &                 &                &                \\
        \texttt{initOperator}, x      & R            &              &                     &                   &                 &                 & W              &                \\
        \texttt{initOperator}, y      &              & R            &                     &                   &                 &                 &                & W              \\
        \texttt{setPayoff}            & R            &              &                     & W                 &                 &                 &                &                \\
        \texttt{updateParams}         & R            & R            & R                   &                   & W               & W               &                &                \\
        \texttt{rollback}, explicit x &              &              &                     & R                 & R               &                 & R              &                \\
        \texttt{rollback}, explicit y &              &              &                     & R                 &                 & R               &                & R              \\
        \texttt{rollback}, implicit x &              &              &                     &                   & R               &                 & R              &                \\
        \texttt{rollback}, implicit y &              &              &                     & W                 &                 & R               &                & R              \\
        \bottomrule
    \end{tabular}
    \caption{Reads and writes in global variables.}
    \label{tbl:rw}
\end{table}

\begin{lstlisting}[caption={bla bla},label={lst:duplicate1}]
REAL myX[outer][numX];
REAL myY[outer][numY];
// ...
for (unsigned i = 0; i < outer; ++i) {
    initGrid(..., i, myX, myY, numX, numY, ...);
    // ...
}
\end{lstlisting}
\begin{lstlisting}[caption={bla bla},label={lst:duplicate2}]
REAL myX[numX];
REAL myY[numY];
// ...
initGrid(..., myX, myY, numX, numY, ...);
// ...
\end{lstlisting}

\subsection{Converting to kernels}

Each line in \autoref{tbl:rw} gets converted to a separate CUDA kernel taking
the parameters where it has read/write operations. Here we need to take special
care of \texttt{rollback} as it has a number of local variables that are shared
across the different subparts of the function. These are \texttt{u} and
\texttt{v}, and their read/write operations are outlined in
\autoref{tbl:rw_rollback}.

This logically represents splitting up the outer loop, i.e.\ doing the
transformation \autoref{lst:splitting1} to \autoref{lst:splitting2}. This
transformation is valid as there are no dependencies across \texttt{outer}
iterations and the results are saved per iteration, except where the results
are identical each iteration as shown in \autoref{sec:eliminate_dup_comp}.

\begin{lstlisting}[caption={bla bla},label={lst:splitting1}]
for (unsigned i = 0; i < outer; ++i) {
    updateparams(...);
    rollback_explicit_x(...);
    // ...
}
\end{lstlisting}
\begin{lstlisting}[caption={bla bla},label={lst:splitting2}]
for (unsigned i = 0; i < outer; ++i) {
    updateparams(...);
}
for (unsigned i = 0; i < outer; ++i) {
    rollback_explicit_x(...);
}
for (unsigned i = 0; i < outer; ++i) {
    // ...
}
\end{lstlisting}

\begin{table}
    \centering
    \begin{tabular}{lccccccc}
        \toprule
        \textbf{Function}             & \texttt{u} & \texttt{v} & \texttt{myResult} & \texttt{myVarX} & \texttt{myVarY} & \texttt{myDxx} & \texttt{myDyy} \\
        \midrule
        \texttt{rollback}, explicit x & W          &            & R                 & R               &                 & R              &                \\
        \texttt{rollback}, explicit y & RW         & W          & R                 &                 & R               &                & R              \\
        \texttt{rollback}, implicit x & RW         &            &                   & R               &                 & R              &                \\
        \texttt{rollback}, implicit y & R          & R          & W                 &                 & R               &                & R              \\
        \bottomrule
    \end{tabular}
    \caption{Reads and writes for \texttt{rollback}.}
    \label{tbl:rw_rollback}
\end{table}

Creating the kernels is done by parallelizing loops as block/thread dimensions
in the kernel. An example is \texttt{rollback}'s ``explicit x'' loop in
\autoref{lst:kernel1} which becomes the CUDA kernel in \autoref{lst:kernel2}.

\begin{lstlisting}[caption={The original ``explicit x'' loop in \texttt{rollback}.},label={lst:kernel1}]
for (unsigned o = 0; o < outer; ++o) {
    // explicit x (rollback)
    for (i = 0; i < numX; i++) {
        for (j = 0; j < numY; j++) {
            // perform computation
        }
    }
}
\end{lstlisting}

\begin{lstlisting}[caption={Converting \texttt{rollback}'s ``explicit x'' loop to a CUDA kernel.},label={lst:kernel2}]
__global__
void rollback_explicit_x_kernel(...) {
    unsigned int tid_y = blockIdx.x*blockDim.x + threadIdx.x;
    unsigned int tid_x = blockIdx.y*blockDim.y + threadIdx.y;

    if (tid_y >= numY || tid_x >= numX)
        return;

    for (unsigned o = 0; o < outer; ++o) {
        // perform computation
    }
}
\end{lstlisting}

Seeing as we cannot pass real multi-dimensional arrays to CUDA kernels, we have
to create one-dimensional arrays and calculate the index manually based on
their coordinates dimension sizes. For this we have created the macros in
\autoref{lst:idx_macro}. This means that if \texttt{a} is a $3 \times 4 \times
5$-dimensional array, we would write \verb|a[IDX3(3,4,5, i,j,k)]| instead of
\verb|a[i][j][k]|.

\begin{lstlisting}[caption={Calculating array indices.},label={lst:idx_macro}]
#define IDX2(DIMX, DIMY, X, Y)           ((X)*(DIMY) + (Y))
#define IDX3(DIMX, DIMY, DIMZ, X, Y, Z)  ((X)*(DIMY)*(DIMZ) + (Y)*(DIMZ) + (Z))
\end{lstlisting}

\subsection{Coalescing memory accesses}

In order to coalesce memory accesses, we restructured a number of the arrays,
to match the loop nesting in our kernels. 

\begin{lstlisting}[caption={\texttt{rollback}'s ``explicit x'' kernel pre-coalescing.},label={lst:coalesce1}]
void rollback_explicit_x_kernel(...) {
    unsigned int tid_y = blockIdx.x*blockDim.x + threadIdx.x;
    unsigned int tid_x = blockIdx.y*blockDim.y + threadIdx.y;

    if (tid_y >= numY || tid_x >= numX)
        return;

    for (unsigned o = 0; o < outer; ++o) {
        // ...
        u[IDX3(outer,numY,numX, o,tid_y,tid_x)] = ...
        // ...
    }
\end{lstlisting}

\begin{lstlisting}[caption={\texttt{rollback}'s ``explicit x'' kernel post-coalescing.},label={lst:coalesce2}]
void rollback_explicit_x_kernel(...) {
    unsigned int tid_y = blockIdx.x*blockDim.x + threadIdx.x;
    unsigned int tid_x = blockIdx.y*blockDim.y + threadIdx.y;

    if (tid_y >= numY || tid_x >= numX)
        return;

    for (unsigned o = 0; o < outer; ++o) {
        // ...
        u[IDX3(outer,numX,numY, o,tid_x,tid_y)] = ...
        // ...
    }
\end{lstlisting}

In order to do this restructuring, we can do a transpose of the two innermost
dimensions. With most of the arrays, we only need them in their transposed
form in order to get good coalesced access, and therefore don't need to
perform any actual transpose operation.

The transpose function is based on the tiled two-dimensional transpose,
performing a segmented transpose, treating each row as a segment.

Furthermore, we split the implicit kernels into two parts, the first
initializing the input vectors to tridag, and the other performing the
actual tridag call. The first kernel is then parallelizable in the x and y
dimensions, allowing us to do nicely coalesced writes. The second kernel is
only parallelizable on one of x and y.

Coalescing memory in all the kernels, without optimizing tridag brought the
running time on the large dataset down from about 25 seconds, to about 16
seconds.

\subsection{Optimizing tridag}

In order to coalesce memory accesses in tridag, we needed to change it to
work on columns instead of rows. We enable doing this by resizing each input
vector to have size \texttt{numZ = max(numX, numY)}, allowing us to access the
element \texttt{i} at index \texttt{i * numZ}. 

\begin{lstlisting}[caption={The call to \texttt{tridag} pre-coalescing},label={lst:tridag1}]
__device__
void tridag(...) {
    ...
    uu[i] = b[i] - beta*c[i-1];
    ...
}

__global__
void rollback_implicit_x_part2_kernel(...) {
    ...
    REAL *myA  =  &a[IDX3(outer,numZ,numZ, tid_outer,tid_y,0)];
    REAL *myB  =  &b[IDX3(outer,numZ,numZ, tid_outer,tid_y,0)];
    ...

    tridag(myA,myB,myC,myU,numX,myU,myYY);
}
\end{lstlisting}

\begin{lstlisting}[caption={The call to \texttt{tridag} post-coalescing},label={lst:tridag2}]
__device__
void tridag(..., int stride) {
    ...
    uu[i*stride] = b[i*stride] - beta*c[(i-1)*stride];
    ...
}

__global__
void rollback_implicit_x_part2_kernel(...) {
    ...
    REAL *myA  =  &a[IDX3(outer,numZ,numZ, tid_outer,0,tid_y)];
    REAL *myB  =  &b[IDX3(outer,numZ,numZ, tid_outer,0,tid_y)];
    ...

    tridag(myA,myB,myC,myU,numX,myU,myYY,numZ);
}
\end{lstlisting}

Performing this coalescing had a large impact on the execution time, lowering
it from about 16 seconds to around 9 seconds on the large dataset.

While we do believe this transformation to be valid, sadly we appear to have
introduced a bug in performing it, making the precision degenerate rapidly
on the large dataset, causing it to not validate, even with our increased
epsilon. We do believe that the execution time matches what we could expect
with the bug fixed, however.

\subsection{Precision problems}

% XXX

\subsection{Speedup}

% XXX

\end{document}
