\documentclass[a4paper]{article}
\usepackage{a4wide}

\usepackage[danish,english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[protrusion=true,expansion=true,final]{microtype}
\usepackage[T1]{fontenc}
\usepackage[sc]{mathpazo}

\usepackage{hyperref}
\usepackage{booktabs}

\usepackage{subfig}
\newcommand{\subfigureautorefname}{\figureautorefname}
\newcommand{\subtableautorefname}{\tableautorefname}
\usepackage[margin=10pt,font=footnotesize,labelfont=bf]{caption}

\title{Programming Massively Parallel Hardware \\ Group Project}
\author{Sebastian Paaske TÃ¸rholm \and Daniel Egeberg}

\begin{document}
\maketitle

\section{Loop accesses}
\label{sec:loop}

\subsection{\texttt{run\_OrigCPU}}

Contains a single loop that can be parallelized by privatizing \texttt{strike} and \texttt{globs}.

\subsection{\texttt{value}}

The loop in this function is not parallelizable. It loops over time steps, and
calls \texttt{rollback}, which uses results from previous time steps. This is
sequential in nature.

\subsection{\texttt{updateParams}}

Already parallelizable as each iteration step is independent.

\subsection{\texttt{rollback}}

Contains four outer loops:
\begin{enumerate}
    \item Already parallelizable as it writes to \texttt{u[j][i]} without reading from \texttt{u}.
    \item Already parallelizable as it writes to \texttt{v[i][j]} and \texttt{u[j][i]} without reading from \texttt{v} or \texttt{u} for different indices.
    \item The inner loop is already parallelizable as it writes to separate indices of \texttt{a}, \texttt{b} and \texttt{c}. The outer loop cannot be parallelized as it calls \texttt{tridag}, which rewrites \texttt{u} based on previous iterations.
    \item The inner loop is already parallelizable as it writes to separate indices of \texttt{a}, \texttt{b}, \texttt{c} and \texttt{y}. The outer loop can, unlike the previous loop, be parallelized by privatizing the aforementioned variables. This is possible because we write to \texttt{myResult} based on \texttt{y} (which is local).
\end{enumerate}

\subsection{\texttt{tridag}}

This can be parallelized by changing the algorithm to the one mentioned in the lectures.


\section{OpenMP implementation}

This is done by privatizing the loop in \texttt{run\_OrigCPU} as described in \autoref{sec:loop}. Doing this we can just add
\begin{verbatim}
#pragma omp parallel for default(shared) schedule(static) if(outer>4)
\end{verbatim}
to the outer loop as suggested.


\section{CUDA implementation}

\subsection{Moving out the time loop}

As we observed in \autoref{sec:loop}, the time loop is sequential, so we want
this as our outermost loop. We start by inlining the \texttt{value} function in
the run function. This enables us to perform this rewrite. From this, we can
observe that \texttt{updateParams} and \texttt{rollback} are the only
operations that depend on the time step. Thus, we can factor out all other
operations in a parallelizable loop by maintaining a separate \texttt{globs}
struct per ``\texttt{outer}''.

\subsection{Converting from STL vectors}

% XXX

\subsection{Eliminate duplicate computations}

% XXX

\subsection{Converting to kernels}

% XXX

\subsection{Coalescing memory accesses}

% XXX

\subsection{Optimizing tridag}

% XXX

\end{document}
